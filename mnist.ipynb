{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "name": "Copy of mnist.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KREE_hxE0Q98"
      },
      "source": [
        "# Feedforward Networks for Handwritten Digit Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW_UBPj70Q99"
      },
      "source": [
        "In this assignment you will learn how to use feedforward neural networks to solve a classical task in machine learning: handwritten digit recognition using images from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). More concretely, you will have to solve the following tasks:\n",
        "\n",
        "1. implement a deep feedforward network that reads a batch of images and predicts the corresponding digits\n",
        "2. train this network using stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GknTTECi0Q99"
      },
      "source": [
        "In order to run the code for this assignment, you need a working installation of [NumPy](http://www.numpy.org). Check whether everything works by running the following code cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWvA1UZ-0Q99"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQY2reQA0Q99"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxEuR7mu0Q99"
      },
      "source": [
        "The MNIST dataset is split into a training set with 60,000 instances and a test set with 10,000 instances. Each instance consists of a greyscale image of a handwritten digit and an integer representing the digit in the image, as labelled by human experts. The digits are scaled and centred on a 28-by-28 pixel canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ePk5Zwg0Q99"
      },
      "source": [
        "The following code will read the training data and the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ir76t6E50Q99",
        "outputId": "2dd8f655-246f-497f-91b5-3e85d4f684be"
      },
      "source": [
        "import mnist_network\n",
        "\n",
        "training_x, training_y = mnist_network.read_training_data()\n",
        "print('Shapes of the training data matrices: {} {}'.format(training_x.shape, training_y.shape))\n",
        "\n",
        "test_x, test_y = mnist_network.read_test_data()\n",
        "print('Shapes of the test data matrices: {} {}'.format(test_x.shape, test_y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes of the training data matrices: (60000, 784) (60000, 10)\n",
            "Shapes of the test data matrices: (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ZQvyMz0Q99"
      },
      "source": [
        "From a Python perspective, each of the two data sets is a pair `(x, y)` of matrices: Each row of `x` is a 784-component vector containing the greyscale values of the pixels in an image as floats between 0 and 1. Each row of `y` is a 10-component one-hot vector representing the digit corresponding to the image. As an example, here is the vector for the first digit in the test data, the digit 7:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q66Vii2I0Q99",
        "outputId": "c3b84009-ce5e-4128-b4a7-abf2fd9cb360"
      },
      "source": [
        "test_y[1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpT9rORT0Q9-"
      },
      "source": [
        "## Task 1: Implement the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH75Y0Rv0Q9-"
      },
      "source": [
        "Your first task is to implement a deep feedforward network that reads a batch of image vectors and predicts the corresponding digits. Your network should conform to the following specification:\n",
        "\n",
        "* one input layer, one output layer, flexible number of hidden layers\n",
        "* activation function for hidden layers: rectified linear unit (ReLU)\n",
        "* activation function for output layer: softmax\n",
        "* error function for gradient computation: categorical cross-entropy\n",
        "\n",
        "To get you started on this task, we provide skeleton code and a number of useful helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7EEPD70Q9-"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "The following cell contains NumPy-based implementations of the ReLU activation function and its derivative (which you should use for the hidden layers of your network), as well as the softmax activation function (for the output layer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S28CrpDD0Q9-"
      },
      "source": [
        "def relu(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return 1 * (x > 0)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=1, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TDajAgP0Q9-"
      },
      "source": [
        "In each case, the argument `x` is a batch of input values, such as `training_x`. The implementation of the softmax function uses a standard trick to improve numerical stability; see [this link](http://stackoverflow.com/questions/34968722/softmax-function-python) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHFhOYkM0Q9-"
      },
      "source": [
        "### Skeleton code\n",
        "\n",
        "To get you started, we provide the following skeleton code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-20MVgUN0Q9-"
      },
      "source": [
        "class Network():\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        self.sizes = sizes\n",
        "        self.ws = [np.random.randn(m, n)*0.01 for m, n in zip(self.sizes[:-1], self.sizes[1:])] \n",
        "        self.bs = [np.random.randn(1, n)*0 for n in self.sizes[1:]]\n",
        "\n",
        "    # forward: computes the output of the network for a batch of input values\n",
        "    def forward(self, x):\n",
        "        hidden_layers_activation_functions = list()\n",
        "        for w,b in zip(self.ws[:-1],self.bs[:-1]):\n",
        "            hidden_layer_activation= relu(np.dot(x, w) + b) \n",
        "            x = hidden_layer_activation\n",
        "            hidden_layers_activation_functions.append(hidden_layer_activation)\n",
        "        # hidden_layers is a list of activation functions for the different layers\n",
        "        output = softmax(np.dot(hidden_layers_activation_functions[-1], self.ws[-1]) + self.bs[-1]) \n",
        "        return output\n",
        "\n",
        "    # pick the digit with the highest probability\n",
        "    def predict(self, x):\n",
        "        return np.argmax(self.forward(x), axis=1)\n",
        "\n",
        "\n",
        "    # backpropagate: computes the network gradients for a batch of input and corresponding output values\n",
        "    def backpropagate(self, x, y):\n",
        "        inputs = x\n",
        "        forward_probabilities = self.forward(np.asarray(x))                     \n",
        "        delta_output =  forward_probabilities - y\n",
        "        # hidden layers' activation functions plus the inputs x\n",
        "        hidden_layers_activation_functions = list()\n",
        "        hidden_layers_activation_functions.append(inputs)\n",
        "        for w,b in zip(self.ws[:-1],self.bs[:-1]):\n",
        "            hidden_layer_activation= relu(np.dot(x, w) + b) \n",
        "            x = hidden_layer_activation\n",
        "            hidden_layers_activation_functions.append(hidden_layer_activation)\n",
        "        #output = softmax(np.dot(hidden_layers_activation_functions[-1], self.ws[-1]) + self.bs[-1]) \n",
        "        #hidden_layers_activation_functions.append(output)\n",
        "        grad_out = np.dot(delta_output.T, hidden_layers_activation_functions[-1]).T\n",
        "        grad_bias_out = np.sum(delta_output, axis=0, keepdims=True)\n",
        "        # delta_layers is a list of delta values for the hidden layers and the output layer\n",
        "        delta_layers = list()\n",
        "        delta_layers.append(delta_output)\n",
        "        grad_bias_layers = list()\n",
        "        grad_layers = list()\n",
        "        weights = self.ws.copy()\n",
        "\n",
        "        for w,b in zip(reversed(self.ws[:-1]), reversed(self.bs[:-1])):\n",
        "            relu_prime_factor = relu_prime(np.dot(hidden_layers_activation_functions[-2], w) + b) \n",
        "            sum_product = np.dot(delta_layers[-1], weights[-1].T)                                   \n",
        "            delta_layer = np.multiply(relu_prime_factor, sum_product)\n",
        "\n",
        "            delta_layers.append(delta_layer)\n",
        "\n",
        "            grad_bias_layer = np.sum(delta_layer, axis=0, keepdims=True)\n",
        "            grad_bias_layers.append(grad_bias_layer)\n",
        "            \n",
        "            x = hidden_layers_activation_functions[-2]\n",
        "\n",
        "            grad_layer = np.dot(x.T, delta_layer)\n",
        "            grad_layers.append(grad_layer)\n",
        "\n",
        "            weights.pop(-1)\n",
        "            hidden_layers_activation_functions.pop(-1)\n",
        "             \n",
        "        grad_layers.reverse()\n",
        "        grad_layers.append(grad_out)\n",
        "        grad_bias_layers.reverse()\n",
        "        grad_bias_layers.append(grad_bias_out)\n",
        "\n",
        "        return grad_layers, grad_bias_layers \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAyHAyMA0Q9-"
      },
      "source": [
        "This code defines a class Network that represents deep feedforward networks. The intended behaviour of the fields and methods of this class is specified below. In the skeleton code, all fields are initialized with, and all methods return, zero matrices of the appropriate shape. In order to obtain a network that meets the requirements, you will have to write code that replaces these placeholders with meaningful values.\n",
        "\n",
        "In your implementation, you may choose to add more fields and/or methods than the ones included in the skeleton code. However, in all of your code, you may only call functions from the NumPy library, but no other library.\n",
        "\n",
        "### Fields\n",
        "\n",
        "**sizes : list(int)**\n",
        "\n",
        "The dimensions of the network layers, from the first (input) layer to the last (output) layer.\n",
        "\n",
        "An example, in a network with 784 units in the input layer, 10 units in the output layer, and 100 units in the (single) hidden layer this field would have the value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3cCq2b0Q9-",
        "outputId": "8bc85def-e568-441c-da7a-aee760216ab6"
      },
      "source": [
        "[784, 100, 10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[784, 100, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBhlpfJ_0Q9-"
      },
      "source": [
        "**ws : list(np.array)**\n",
        "\n",
        "The weight matrices of the network, where the matrix at index $i$ holds the weights of the connections from layer $i$ to layer $i+1$. As an example, if the shape of layer&nbsp;0 is $(784, 100)$ and the shape of layer&nbsp;1 is $(100, 10)$, then `ws[0]` will have shape $(100, 10)$.\n",
        "\n",
        "**bs : list(np.array)**\n",
        "\n",
        "The bias vectors of the network, where the vector at index $i$ holds the biases for layer $i+1$. As an example, `bs[0]` is the bias vector of layer&nbsp;1. Note that there are no biases for the input layer (layer&nbsp;0).\n",
        "\n",
        "### Initialization\n",
        "\n",
        "Initialize the weights and biases of the network. Note that in the starter code, both weights and biases are initialized using zeros.\n",
        "\n",
        "**sizes : list(int)**\n",
        "\n",
        "The dimensions of the network layers, from the first (input) layer to the last (output) layer.\n",
        "\n",
        "As an example, the following code creates a network with 784 units in the input layer, 10 units in the output layer, and 100 units in the (single) hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-5UzjH00Q9-"
      },
      "source": [
        "net = Network([784, 100, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1LJs9P-0Q9-"
      },
      "source": [
        "### forward\n",
        "\n",
        "Computes the output of the network for a batch of input values.\n",
        "\n",
        "**x : np.array**\n",
        "\n",
        "A batch of input values, such as `training_x`.\n",
        "\n",
        "**Returns:** The output of the network for the specified input. This will be an array of shape $(m, n)$ where $m$ is the number of rows in the input batch, and $n$ is the size of the last layer in the network. In the starter code, the method returns an array of all zeros.\n",
        "\n",
        "### predict\n",
        "\n",
        "Predicts the digits for a batch of input values.\n",
        "\n",
        "**x : np.array**\n",
        "\n",
        "A batch of input values, such as `test_x`.\n",
        "\n",
        "**Returns:** The digits predicted for the specified input. This will be an array of shape $(m, 1)$ where $m$ is the number of rows in the input batch $x$. In the starter code, the method returns an array of all zeros.\n",
        "\n",
        "### backpropagate\n",
        "\n",
        "Computes the network gradients for a batch of input and corresponding output values. Note that in the context of this assignment, the gradients should be computed relative to categorical cross-entropy as the error function.\n",
        "\n",
        "**x : np.array**\n",
        "\n",
        "A batch of input values, such as `training_x`.\n",
        "\n",
        "**y : np.array**\n",
        "\n",
        "A batch of corresponding output values, such as `training_y`.\n",
        "\n",
        "**Returns:** A list of pairs of the form $(\\nabla w, \\nabla b)$, one for each non-input layer of the network, where the first component of each pair is the average gradient for the weights of the connections coming into the layer and the second component is the average gradient for the biases at the layer. In the starter code, the method returns a list of zero gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTjVj_nB0Q9-"
      },
      "source": [
        "## Task 2: Train your network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIZqaX1p0Q9-"
      },
      "source": [
        "Once you have completed the Network class, your second task is to write code to train the network using stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdRs_zR0Q9-"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "The function in the next code cell will sample minibatches from an array `x` of input values and a corresponding array `y` of output values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChCCainL0Q9-"
      },
      "source": [
        "def minibatches(x, y, batch_size):\n",
        "    random_indices = np.random.permutation(np.arange(x.shape[0]))\n",
        "    for i in range(0, x.shape[0] - batch_size + 1, batch_size):\n",
        "        batch_indices = random_indices[i:i+batch_size]\n",
        "        yield x[batch_indices], y[batch_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIgmlkr0Q9-"
      },
      "source": [
        "The next function computes the test error rate of a network on a batch of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63ILHj-0Q9-"
      },
      "source": [
        "def evaluate(net):\n",
        "    return np.mean(net.predict(test_x) != np.argmax(test_y, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MluQGv3Q0Q9-"
      },
      "source": [
        "### Skeleton code\n",
        "\n",
        "The following cell contains skeleton code for the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ncK8SBw0Q9-"
      },
      "source": [
        "# train_sgd: the training algorithm\n",
        "def train_sgd(net, x, y, n_epochs, batch_size, eta=0.007):\n",
        "    for t in range(n_epochs):\n",
        "        for batch_x, batch_y in minibatches(x, y, batch_size):\n",
        "            grad_layers, grad_bias_layers = net.backpropagate(batch_x,batch_y)\n",
        "            for i in range(len(grad_layers)):\n",
        "                net.ws[i] = net.ws[i] - np.dot(eta, grad_layers[i])\n",
        "                net.bs[i] = net.bs[i] - np.dot(eta, grad_bias_layers[i])\n",
        "        print(\"epoch = {}, test error rate = {:.4f}\".format(t, evaluate(net)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfKHB5a90Q9-"
      },
      "source": [
        "The intended meaning of the various parameters is as follows:\n",
        "\n",
        "**x : np.array**\n",
        "\n",
        "A batch of input values, such as `training_x`.\n",
        "\n",
        "**y : np.array**\n",
        "\n",
        "A batch of corresponding output values, such as `training_y`.\n",
        "\n",
        "**n_epochs : int**\n",
        "\n",
        "The number of iterations over the training data (&lsquo;epochs&rsquo;).\n",
        "\n",
        "**batch_size : int**\n",
        "\n",
        "The number of input values per minibatch.\n",
        "\n",
        "**eta : float**\n",
        "\n",
        "The learning rate in the stochastic gradient descent update step.\n",
        "\n",
        "### Intended usage\n",
        "\n",
        "To see how the training code is intended to be used, here is how you set up a network and train it on the training data for 2&nbsp;iterations with minibatch size&nbsp;30 and the default learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxzNR1QR0Q9-",
        "outputId": "00bb117f-0775-40b7-c2a1-689698ef2e6b"
      },
      "source": [
        "net = Network([784, 300, 120, 10])\n",
        "train_sgd(net, training_x, training_y, 10, 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0, test error rate = 0.0402\n",
            "epoch = 1, test error rate = 0.0317\n",
            "epoch = 2, test error rate = 0.0279\n",
            "epoch = 3, test error rate = 0.0230\n",
            "epoch = 4, test error rate = 0.0223\n",
            "epoch = 5, test error rate = 0.0197\n",
            "epoch = 6, test error rate = 0.0209\n",
            "epoch = 7, test error rate = 0.0169\n",
            "epoch = 8, test error rate = 0.0183\n",
            "epoch = 9, test error rate = 0.0192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGKN8Pva0Q9-"
      },
      "source": [
        "### Performance goal\n",
        "\n",
        "Once you have a working network and training algorithm, you can compare the error rate of your network to the results on the [MNIST website](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "**To get credit for this assignment, your network must achieve a test error rate of less than 2% at least once during the first 10 epochs of training.**\n",
        "\n",
        "To tune your network, you can play around with the various training parameters: number of epochs, minibatch size, and learning rate. In addition to that, you can also make more substantial changes such as the following:\n",
        "\n",
        "* Make the network wider (increase the size of a layer) or deeper (add more layers).\n",
        "* Implement a different initialization strategy.\n",
        "* Implement a regularization method or dropout.\n",
        "* Implement an optimization algorithm with an adaptive learning rate, such as RMSProp or Adam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_x0XgGs0Q9-"
      },
      "source": [
        "## How to submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbNFsVY0Q9-"
      },
      "source": [
        "When you have reached the performance goal, send this notebook to Marco to receive credit for the assignment. The notebook must be self-contained and must run without error.\n",
        "\n",
        "In addition to your code, you are asked to submit a short text (less than 500&nbsp;words) in which you reflect on what you have done. Which specific choices did you make when tuning your network? How did these choices affect performance? You can enter your text in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQd3rbnm0Q9-"
      },
      "source": [
        "### Report\n",
        "\n",
        "In this lab, a neural network with an input layer and three hidden layers was created. In order to get less than 2% error rate, tuning was necessary. The learning rate should not be too large or too small. The weights and bias initialization is very important and different approaches should be tested in order to get the small error rate. The minibatch size is kept small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2noa_XFk0Q9-"
      },
      "source": [
        "*Good luck!*"
      ]
    }
  ]
}